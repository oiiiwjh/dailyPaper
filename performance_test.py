#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬ - æµ‹è¯•ä¼˜åŒ–åçš„æ€§èƒ½æ”¹è¿›
"""

import time
import json
from pathlib import Path
from scripts.fetch_papers import PaperFetcher
from scripts.generate_html import HTMLGenerator


def test_paper_classification():
    """æµ‹è¯•è®ºæ–‡åˆ†ç±»æ€§èƒ½"""
    print("=" * 60)
    print("æµ‹è¯•è®ºæ–‡åˆ†ç±»æ€§èƒ½")
    print("=" * 60)
    
    fetcher = PaperFetcher()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path("data/papers.json")
    if not data_file.exists():
        print("æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶")
        return
    
    with open(data_file, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    # æµ‹è¯•åˆ†ç±»æ€§èƒ½ï¼ˆåªæµ‹è¯•å‰100ç¯‡ä»¥èŠ‚çœæ—¶é—´ï¼‰
    test_papers = papers[:100]
    
    start_time = time.time()
    for paper in test_papers:
        tags = fetcher.classify_paper(paper)
    end_time = time.time()
    
    elapsed = end_time - start_time
    print(f"âœ… åˆ†ç±» {len(test_papers)} ç¯‡è®ºæ–‡è€—æ—¶: {elapsed:.3f} ç§’")
    print(f"   å¹³å‡æ¯ç¯‡: {elapsed/len(test_papers)*1000:.2f} æ¯«ç§’")


def test_venue_extraction():
    """æµ‹è¯•ä¼šè®®æœŸåˆŠæå–æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ä¼šè®®æœŸåˆŠæå–æ€§èƒ½")
    print("=" * 60)
    
    fetcher = PaperFetcher()
    
    test_comments = [
        "Accepted to CVPR 2025",
        "ICML 2024",
        "preprint",
        None,
        "IEEE Transactions on Pattern Analysis",
        "To appear in NeurIPS 2024",
        "ICCV 2023 Oral Presentation"
    ] * 100  # é‡å¤100æ¬¡ä»¥è·å¾—æ›´å‡†ç¡®çš„è®¡æ—¶
    
    start_time = time.time()
    for comment in test_comments:
        venue = fetcher.extract_venue_from_comment(comment)
    end_time = time.time()
    
    elapsed = end_time - start_time
    print(f"âœ… æå– {len(test_comments)} ä¸ªä¼šè®®ä¿¡æ¯è€—æ—¶: {elapsed:.3f} ç§’")
    print(f"   å¹³å‡æ¯ä¸ª: {elapsed/len(test_comments)*1000:.2f} æ¯«ç§’")


def test_html_generation():
    """æµ‹è¯•HTMLç”Ÿæˆæ€§èƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•HTMLç”Ÿæˆæ€§èƒ½")
    print("=" * 60)
    
    generator = HTMLGenerator()
    generator.load_papers()
    
    if not generator.papers:
        print("æœªåŠ è½½åˆ°è®ºæ–‡æ•°æ®")
        return
    
    # æµ‹è¯•HTMLç”Ÿæˆ
    start_time = time.time()
    html = generator.generate_papers_html()
    end_time = time.time()
    
    elapsed = end_time - start_time
    print(f"âœ… ç”Ÿæˆ {len(generator.papers)} ç¯‡è®ºæ–‡çš„HTMLè€—æ—¶: {elapsed:.3f} ç§’")
    print(f"   HTMLå¤§å°: {len(html) / 1024 / 1024:.2f} MB")


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•\n")
    
    try:
        test_paper_classification()
        test_venue_extraction()
        test_html_generation()
        
        print("\n" + "=" * 60)
        print("âœ¨ æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print("\nä¸»è¦ä¼˜åŒ–ç‚¹ï¼š")
        print("1. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œé¿å…é‡å¤ç¼–è¯‘")
        print("2. ç¼“å­˜å°å†™æ–‡æœ¬ï¼Œé¿å…é‡å¤å­—ç¬¦ä¸²æ“ä½œ")
        print("3. åˆ†ç±»æ—¶ä½¿ç”¨æ—©æœŸé€€å‡ºï¼Œæ‰¾åˆ°åŒ¹é…åç«‹å³è·³è¿‡")
        print("4. HTMLç”Ÿæˆä½¿ç”¨joinè€Œä¸æ˜¯å­—ç¬¦ä¸²æ‹¼æ¥")
        print("5. CSS/JSç”Ÿæˆå‰æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¹å˜ï¼Œé¿å…ä¸å¿…è¦çš„å†™å…¥")
        print("6. JavaScriptç«¯ç¼“å­˜DOMæ–‡æœ¬å†…å®¹ï¼Œé¿å…é‡å¤è°ƒç”¨textContent")
        print("7. æœç´¢è¾“å…¥æ·»åŠ 300msé˜²æŠ–ï¼Œå‡å°‘ä¸å¿…è¦çš„è¿‡æ»¤æ“ä½œ")
        print("8. ä½¿ç”¨CSSç±»éšè—å…ƒç´ è€Œéå†…è”æ ·å¼ï¼Œæå‡æ¸²æŸ“æ€§èƒ½")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
